version: v1.1
Last updated: 2025-05-01

# âš ï¸ Limitations of the LoRA Fine-Tuned Model (OpenLLaMA 3B)

This file summarizes known blind spots and challenges from testing the LoRA-fine-tuned version of the OpenLLaMA 3B model on structured prompts for late blight risk classification.

---

## ğŸš« Known Issues

### 1. Moderate-Class Confusion
- The model struggles to identify borderline "Moderado" cases.
- Often classifies them as either Low or High depending on temperature more than RH duration.

### 2. Prompt Sensitivity
- The output shifts depending on small wording changes (e.g., â€œhigh humidityâ€ vs. â€œ85% humidityâ€).
- Structured prompts with `Riesgo: <mask>` improved performance but remain sensitive.

### 3. Overconfidence
- Frequently returns one dominant class, often "Bajo," even when inputs are ambiguous.
- Confidence scores are not explicitly expressed by the model.

### 4. RH Duration Handling
- RH is treated as binary (above or below threshold), but the model doesnâ€™t assess multi-day sequences well.
- Cannot distinguish â€œ82% for 6hâ€ from â€œ82% for 2 daysâ€ without very specific phrasing.

---

## ğŸ¤– Model Role in the Current GPT

- The LoRA model is **not actively used inside this GPT**.
- This GPT applies expert rules from `rules_late_blight.md` instead.
- However, the limitations above justify building a hybrid fallback system where both models cross-validate.
